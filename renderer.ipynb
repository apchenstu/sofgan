{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, random\n",
    "import torch, os, argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.stats import norm\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.model_seg_input import Generator\n",
    "from modules.BiSeNet import BiSeNet\n",
    "from utils import *\n",
    "from modules.model_seg_input import scatter as scatter_model\n",
    "\n",
    "import sys,os\n",
    "root = os.path.abspath('.')\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IDList = [np.arange(17).tolist(),[0],[1,4,5,9,12],[15],[6,7,8,3],[11,13,14,16,10]]\n",
    "# IDList = [[0],[1,4,5,9,12],[15],[2,3,6,7,8,10,11,13,14,16]]\n",
    "groupName = ['Global','Background','Complexion','Hair','Eyes & Mouth','Wearings']\n",
    "def scatter_to_mask(segementation, out_num=1,add_whole=True,add_flip=False,region=None):\n",
    "    segementation = scatter_model(segementation)\n",
    "    masks = []\n",
    "\n",
    "    if None == region:\n",
    "        if add_whole:\n",
    "            mask = torch.sum(segementation, dim=1, keepdim=True).clamp(0.0, 1.0)\n",
    "            masks.append(torch.cat((mask, 1.0 - mask), dim=1))\n",
    "        if add_flip:\n",
    "            masks.append(torch.cat((1.0 - mask, mask), dim=1))\n",
    "\n",
    "\n",
    "        for i in range(out_num - add_whole - add_flip):\n",
    "            idList = IDList[i]\n",
    "            mask = torch.sum(segementation[:, idList], dim=1, keepdim=True).clamp(0.0, 1.0)\n",
    "            masks.append(torch.cat((1.0 - mask, mask), dim=1))\n",
    "    else:\n",
    "        for item in region:\n",
    "            idList = IDList[item]\n",
    "            mask = torch.sum(segementation[:, idList], dim=1, keepdim=True).clamp(0.0, 1.0)\n",
    "            masks.append(torch.cat((1.0 - mask, mask), dim=1))\n",
    "    masks = torch.cat(masks, dim=0)\n",
    "    return masks\n",
    "\n",
    "def make_noise(batch, styles_dim, style_repeat, latent_dim, n_noise, device):\n",
    "    noises = torch.randn(n_noise, batch, styles_dim, latent_dim, device=device).repeat(1, 1, style_repeat, 1)\n",
    "    return noises\n",
    "\n",
    "def mixing_noise(batch, latent_dim, prob, device, unbine=True):\n",
    "    n_noise = 1\n",
    "    style_dim = 2 if random.random() < prob else 1\n",
    "    style_repeat = 2 // style_dim  # if prob>0 else 1\n",
    "    styles = make_noise(batch, style_dim, style_repeat, latent_dim, n_noise, device)\n",
    "    return styles.unbind(0) if unbine else styles\n",
    "\n",
    "\n",
    "def sample_styles_with_miou(seg_label, num_style, mixstyle=0, truncation=0.9, batch_size=4, descending=False):\n",
    "    times = 0\n",
    "    in_batch = seg_label.shape[0]\n",
    "    if in_batch == 1:\n",
    "        batch = batch_size\n",
    "        seg_label = seg_label.repeat(batch, 1, 1, 1)\n",
    "    else:\n",
    "        batch = in_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        styles_miou, count, mious = [], 0, []\n",
    "        while count < num_style:\n",
    "            styles = mixing_noise(batch // in_batch, args.latent, mixstyle, device, unbine=False)\n",
    "            styles = to_w_style(generator.style_map_norepeat, styles, latent_av, trunc_psi=truncation)\n",
    "            styles = torch.cat(styles, dim=0)\n",
    "            w_latent = generator.style_map([styles], to_w_space=False)\n",
    "\n",
    "            if in_batch > 1:\n",
    "                w_latent = w_latent.repeat(batch, 1, 1)\n",
    "\n",
    "            img, _, _, _ = generator(w_latent, return_latents=False, condition_img=seg_label, input_is_latent=True,\n",
    "                                     noise=noise)\n",
    "            img = img.clamp(-1.0, 1.0)\n",
    "            img = F.interpolate(img, size=(512, 512), mode='bilinear')\n",
    "\n",
    "            segmap = bisNet(img)[0]\n",
    "            segmap = F.interpolate(segmap, size=seg_label.shape[2:], mode='bilinear')\n",
    "            segmap = id_remap(torch.argmax(segmap, dim=1, keepdim=True))\n",
    "\n",
    "            thread = 0.46\n",
    "            if times > 15:\n",
    "                thread = 0.42\n",
    "            if times > 20:\n",
    "                thread = 0.35\n",
    "            if times > 30:\n",
    "                thread = 0.\n",
    "\n",
    "            miou = mIOU(segmap, seg_label)\n",
    "            miou = miou.min() if in_batch > 1 else miou\n",
    "            mask = (miou > thread).tolist()\n",
    "\n",
    "            times += 1\n",
    "            if np.sum(mask) == 0:\n",
    "                continue\n",
    "\n",
    "            if in_batch > 1 and mask:\n",
    "                mious.append(miou.view(-1, 1))\n",
    "                styles_miou.append(w_latent[[0]])\n",
    "                count += 1\n",
    "            else:\n",
    "                mious.append(miou[mask])\n",
    "                if len(mask) == w_latent.shape[0]:\n",
    "                    styles_miou.append(w_latent[mask])\n",
    "                else:\n",
    "                    styles_miou.append(\n",
    "                        w_latent.view(-1, 2, w_latent.shape[-2], w_latent.shape[-1])[mask])  # old need this\n",
    "                count += np.sum(mask)\n",
    "\n",
    "    mious = torch.cat(mious, dim=0).view(-1)\n",
    "    mious, indices = torch.sort(mious, descending=descending)\n",
    "    styles_miou = torch.cat(styles_miou, dim=0)[indices]\n",
    "    return styles_miou[:num_style]\n",
    "\n",
    "def initFaceParsing(n_classes=20):\n",
    "    net = BiSeNet(n_classes=n_classes)\n",
    "    net.cuda()\n",
    "    net.load_state_dict(torch.load('./ckpts/segNet-20Class.pth'))\n",
    "    net.eval()\n",
    "    to_tensor = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "    ])\n",
    "    return net, to_tensor\n",
    "\n",
    "\n",
    "def parsing_img(bisNet, image, to_tensor, argmax=True):\n",
    "    with torch.no_grad():\n",
    "        img = to_tensor(image)\n",
    "        img = torch.unsqueeze(img, 0).cuda()\n",
    "        segmap = bisNet(img)[0]\n",
    "        if argmax:\n",
    "            segmap = segmap.argmax(1, keepdim=True)\n",
    "        segmap = id_remap(segmap)\n",
    "    return img, segmap\n",
    "\n",
    "\n",
    "def auto_crop_img(image, detector=None, inv_pad=2):\n",
    "    if detector is None:\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    dets = detector(image, 1)\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    faces = []\n",
    "    for i, d in enumerate(dets):\n",
    "        left, right, top, bottom = d.left(), d.right(), d.top(), d.bottom()\n",
    "        width_crop = right - left\n",
    "        pad = min(w - right, left, top, h - bottom, width_crop // inv_pad)\n",
    "\n",
    "        top = max(top - int(pad * 1.5), 0)\n",
    "        faces.append(image[top:top + width_crop + 2 * pad, left - pad:right + pad])\n",
    "    return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-i', '--input', type=str)\n",
    "parser.add_argument('-o', '--output', type=str)\n",
    "parser.add_argument('-batch_size', type=int,default=4)\n",
    "parser.add_argument('--resolution', type=int, default=1024)\n",
    "parser.add_argument('--nrows', type=int, default=6)\n",
    "parser.add_argument('--ckpt', type=str, default=None)\n",
    "parser.add_argument('--channel_multiplier', type=int, default=2)\n",
    "parser.add_argument('--with_rgb_input', action='store_true')\n",
    "parser.add_argument('--with_local_style', action='store_true')\n",
    "parser.add_argument('--condition_dim', type=int, default=0)\n",
    "parser.add_argument('--styles_path', type=str, default=None)\n",
    "parser.add_argument('--MODE', type=int, default=0)\n",
    "parser.add_argument('--miou_filter', action='store_true')\n",
    "parser.add_argument('--truncation', type=float, default=0.7)\n",
    "parser.add_argument('--with_seg_fc', action='store_true')\n",
    "\n",
    "cmd = f'-i ./dataset/video -o ./result/mv-obama/ \\\n",
    "--ckpt ./ckpts/generator.pt \\\n",
    "--resolution 1024  --MODE 2 --miou_filter --truncation 0.7'\n",
    "args = parser.parse_args(cmd.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define networks\n",
    "args.latent = 512\n",
    "args.n_mlp = 8\n",
    "args.condition_path = args.input\n",
    "generator = Generator(args).eval().to(device)\n",
    "\n",
    "ckpt = torch.load(args.ckpt)\n",
    "generator.load_state_dict(ckpt['g_ema'])\n",
    "\n",
    "batch_size = 4\n",
    "latent_av = cal_av(generator, batch_size, args.latent)\n",
    "\n",
    "# face parser\n",
    "bisNet, to_tensor = initFaceParsing()\n",
    "\n",
    "del ckpt\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-38211c3a030e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsing_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbisNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mseg_label_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvis_condition_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mseg_label_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_label_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-956223c7df2d>\u001b[0m in \u001b[0;36mparsing_img\u001b[0;34m(bisNet, image, to_tensor, argmax)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0msegmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbisNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "img_path = './example/Harry.jpg'# path to the source image folder\n",
    "save_path = './example/test.png'\n",
    "auto_crop = False # you need to center crop the image if you are using your own photos; please set false if image comes from FFHQ or CelebA\n",
    "miou_filter = True # set true if you want to filter style with the miou\n",
    "n_styles = 3\n",
    "resolution_vis = 1024 # image resolution to save \n",
    "save_as_video = True\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    noise = [getattr(generator.noises, f'noise_{i}') for i in range(generator.num_layers)]\n",
    "\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    if auto_crop:\n",
    "        import dlib\n",
    "        faces = auto_crop_img(np.array(img))\n",
    "        img = Image.fromarray(faces[0])\n",
    "        \n",
    "    img, seg_label = parsing_img(bisNet, img.resize((512, 512)), to_tensor)\n",
    "    seg_label_rgb = vis_condition_img(seg_label)\n",
    "    seg_label_rgb = F.interpolate(seg_label_rgb, (args.resolution, args.resolution), mode='bilinear', align_corners=True)\n",
    "\n",
    "    try:\n",
    "        tqdm._instances.clear() \n",
    "    except Exception:     \n",
    "        pass\n",
    "        \n",
    "    if not save_as_video:\n",
    "        if miou_filter:\n",
    "            w_latent = sample_styles_with_miou(seg_label, n_styles * 2, mixstyle=mixstyle,\n",
    "                                           truncation=args.truncation, batch_size=args.batch_size)\n",
    "        else:\n",
    "            mixstyle = 0.0\n",
    "            styles = mixing_noise(n_styles, args.latent, mixstyle, device, unbine=False)\n",
    "            styles = to_w_style(generator.style_map_norepeat, styles, latent_av, trunc_psi=args.truncation)\n",
    "            styles = torch.cat(styles, dim=0)\n",
    "            w_latent = generator.style_map([styles], to_w_space=False)\n",
    "            w_latent = w_latent.view(-1,2,w_latent.shape[-2],w_latent.shape[-1])\n",
    "    \n",
    "        result = [F.interpolate(img.cpu(),(resolution_vis,resolution_vis), mode='bilinear', align_corners=True)]\n",
    "        for j in tqdm(range(n_styles)):\n",
    "            fake_img, _, _, _ = generator(w_latent[j], return_latents=False, condition_img=seg_label, \\\n",
    "                                          input_is_latent=True, noise=noise)\n",
    "            result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis), \n",
    "                                        mode='bilinear', align_corners=True))\n",
    "\n",
    "        result = torch.cat(result, dim=0)\n",
    "        utils.save_image(result, save_path,nrow=n_styles+1,normalize=True,range=(-1, 1),padding = 2)\n",
    "    else:\n",
    "        nrows,ncols = 1, 2\n",
    "        width_pad, height_pad = 2 * (ncols + 1), 2 * (nrows + 1) \n",
    "        fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "        out = cv2.VideoWriter(f'{save_path[:-4]}.mp4', fourcc,\n",
    "                              20, (resolution_vis * ncols + width_pad, resolution_vis * nrows + height_pad))\n",
    "\n",
    "        img = F.interpolate(img.cpu(),(resolution_vis,resolution_vis), mode='bilinear', align_corners=True)\n",
    "        w_latents = sample_styles_with_miou(seg_label, n_styles, mixstyle=0.0, truncation=args.truncation, batch_size=args.batch_size,descending=True)[:,0]\n",
    "        style_masks = scatter_to_mask(seg_label, len(groupName), add_flip=False, add_whole=False)\n",
    "        \n",
    "        w_latent_nexts = []\n",
    "        for i_style in tqdm(range(len(groupName))):\n",
    "\n",
    "            regions = list(range(n_styles)) + [0]\n",
    "\n",
    "            for j,frame in enumerate(range(1,len(regions))):\n",
    "\n",
    "                if 0 == regions[frame - 1]: # first style\n",
    "                    w_latent_last, w_latent_next = w_latents[:1], w_latents[[frame]]\n",
    "                elif 0 == regions[frame]:# last style\n",
    "                    w_latent_last, w_latent_next = w_latent_next.clone(), w_latents[:1]\n",
    "                else:\n",
    "                    w_latent_last = w_latent_next.clone()\n",
    "                    w_latent_next = w_latents[[frame]].clone()\n",
    "\n",
    "\n",
    "                frame_sub_count = 40 if i_style<4 else 30\n",
    "                cdf_scale = 1.0 / (1.0 - norm.cdf(-frame_sub_count // 2, 0, 6) * 2)\n",
    "                for frame_sub in range(-frame_sub_count // 2, frame_sub_count // 2 + 1):\n",
    "\n",
    "                    weight = (norm.cdf(frame_sub, 0, 6) - norm.cdf(-frame_sub_count // 2, 0, 6)) * cdf_scale\n",
    "\n",
    "                    w_latent_current = (1.0 - weight) * w_latent_last + weight * w_latent_next\n",
    "                    w_latent_current = torch.cat((w_latents[:1],w_latent_current),dim=0)\n",
    "\n",
    "\n",
    "                    # first row\n",
    "                    result = [img]\n",
    "                    w_latent_current_in = w_latent_current.view(-1, 18, 512)\n",
    "                    fake_img, _, _, _ = generator(w_latent_current_in, return_latents=False,\n",
    "                                                  condition_img=seg_label, \\\n",
    "                                                  input_is_latent=True, noise=noise,\n",
    "                                                  style_mask=style_masks[[i_style]])\n",
    "                    result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis)\n",
    "                                               , mode='bilinear', align_corners=True))\n",
    "\n",
    "                    result = torch.cat(result, dim=0)\n",
    "                    result = (utils.make_grid(result, nrow=ncols) + 1) / 2 * 255\n",
    "                    result = (result.detach().numpy()[::-1]).transpose((1, 2, 0))\n",
    "                    out.write(result.astype('uint8'))\n",
    "        out.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [01:37<00:00,  4.09it/s]\n"
     ]
    }
   ],
   "source": [
    "video_path = './example/faceCap.avi'# path to the source image folder\n",
    "save_path = './example/faceCap-restyle.mp4'\n",
    "auto_crop = False # you need to center crop the image if you are using your own photos; please set false if image is from FFHQ or CelebA\n",
    "resolution_vis = 512 # image resolution to save \n",
    "save_as_video = True\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "nrows,ncols = 2, 2\n",
    "width_pad, height_pad = 2 * (ncols + 1), 2 * (nrows + 1) \n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "out = cv2.VideoWriter(f'{save_path[:-4]}.mp4', fourcc,\n",
    "                      20, (resolution_vis * ncols + width_pad, resolution_vis * nrows + height_pad))\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    noise = [getattr(generator.noises, f'noise_{i}') for i in range(generator.num_layers)]\n",
    "    \n",
    "    mixstyle = 0.0\n",
    "    styles = mixing_noise(nrows*ncols, args.latent, mixstyle, device, unbine=False)\n",
    "    styles = to_w_style(generator.style_map_norepeat, styles, latent_av, trunc_psi=args.truncation)\n",
    "    styles = torch.cat(styles, dim=0)\n",
    "    w_latent = generator.style_map([styles], to_w_space=False)\n",
    "    w_latent = w_latent.view(-1,2,w_latent.shape[-2],w_latent.shape[-1])\n",
    "\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    success, img = cap.read()\n",
    "    \n",
    "    try:\n",
    "        tqdm._instances.clear() \n",
    "    except Exception:     \n",
    "        pass\n",
    "    for _ in tqdm(range(length-1)):\n",
    "\n",
    "        if auto_crop:\n",
    "            import dlib\n",
    "            faces = auto_crop_img(img[...,::-1])#bgr -> rgb\n",
    "            img = Image.fromarray(faces[0])\n",
    "        else:\n",
    "            img = Image.fromarray(img[...,::-1])\n",
    "\n",
    "        # you may need a facial landmark detector to remove the jitter on the eyes region\n",
    "        # we provide a more stable video parser, you can find it in the readme.\n",
    "        img, seg_label = parsing_img(bisNet, img.resize((512, 512)), to_tensor)\n",
    "        seg_label_rgb = vis_condition_img(seg_label)\n",
    "        seg_label_rgb = F.interpolate(seg_label_rgb, (args.resolution, args.resolution), mode='bilinear', align_corners=True)\n",
    "        \n",
    "\n",
    "        result = [F.interpolate(img.cpu(),(resolution_vis,resolution_vis))]\n",
    "        for j in range(nrows*ncols-1):\n",
    "            fake_img, _, _, _ = generator(w_latent[j], return_latents=False, condition_img=seg_label, \\\n",
    "                                          input_is_latent=True, noise=noise)\n",
    "            result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis)))\n",
    "\n",
    "        result = torch.cat(result, dim=0)\n",
    "        result = (utils.make_grid(result, nrow=nrows) + 1) / 2 * 255\n",
    "        result = result.numpy()[::-1].transpose((1, 2, 0)).astype('uint8')\n",
    "        out.write(result)\n",
    "        result = []\n",
    "        success, img = cap.read()\n",
    "        \n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# free-viewpoint protrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT embedding] variable.\n",
      "[INIT renderer] FC, with renderer = FC\n",
      "DONE Load model.\n",
      "DONE Load ckpt.\n",
      "DONE Build sampling space.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from modules.sof.utils.seg_sampler import FaceSegSampler\n",
    "\n",
    "img_size = 128\n",
    "num_instances = 1000\n",
    "num_poses = 100\n",
    "sample_mode = 'spiral'\n",
    "radius = 4.5\n",
    "\n",
    "seg_sampler = FaceSegSampler(\n",
    "    model_path='./ckpts/epoch_0250_iter_050000.pth', \n",
    "    img_size=512, \n",
    "    sample_mode='spiral',\n",
    "    sample_radius=radius\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samping spiral poses:  (25, 512, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.74it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = './example/fvv.mp4'\n",
    "resolution_vis = 512 # image resolution to save \n",
    "nrows,ncols = 2, 2\n",
    "width_pad, height_pad = 2 * (ncols + 1), 2 * (nrows + 1) \n",
    "n_feames = 120\n",
    "\n",
    "# sampling instance embedding\n",
    "smp_ins = torch.from_numpy(seg_sampler.gmm.sample(1)[0]).float()\n",
    "\n",
    "# sampling poses\n",
    "look_at = np.asarray([0, 0.1, 0.0])\n",
    "cam_center =  np.asarray([0, 0.1, 4.5])\n",
    "smp_poses = seg_sampler.sample_pose(\n",
    "    cam_center, look_at, \n",
    "    num_samples=n_feames, emb=smp_ins)\n",
    "print('Samping spiral poses: ', smp_poses.shape)\n",
    "\n",
    "# generate images\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "out = cv2.VideoWriter(\n",
    "    f'{save_path[:-4]}_spiral.mp4', fourcc,\n",
    "    20, (resolution_vis * ncols + width_pad, resolution_vis * nrows + height_pad))\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_styles = nrows*ncols\n",
    "    seg_label = id_remap(torch.from_numpy(smp_poses[:1]).float()).to(device)\n",
    "    noise = [getattr(generator.noises, f'noise_{i}') for i in range(generator.num_layers)]\n",
    "    w_latents = sample_styles_with_miou(\n",
    "            seg_label, n_styles, mixstyle=0.0, truncation=args.truncation, batch_size=args.batch_size,descending=True)\n",
    "\n",
    "    try:\n",
    "        tqdm._instances.clear() \n",
    "    except Exception:     \n",
    "        pass\n",
    "    for seg_label in tqdm(smp_poses):\n",
    "        \n",
    "        seg_label = id_remap(torch.from_numpy(seg_label).float()[None,None]).to(device)\n",
    "        \n",
    "        seg_label_rgb = vis_condition_img(seg_label)\n",
    "        seg_label_rgb = F.interpolate(seg_label_rgb, (resolution_vis, resolution_vis), mode='bilinear', align_corners=True)\n",
    "        result = [seg_label_rgb]\n",
    "        for j in range(nrows*ncols-1):\n",
    "            fake_img, _, _, _ = generator(  w_latents[j], return_latents=False,\n",
    "                                            condition_img=seg_label, \\\n",
    "                                            input_is_latent=True, noise=noise)\n",
    "        \n",
    "            result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis)))\n",
    "\n",
    "        result = torch.cat(result, dim=0)\n",
    "        result = (utils.make_grid(result, nrow=nrows) + 1) / 2 * 255\n",
    "        result = result.numpy()[::-1].transpose((1, 2, 0)).astype('uint8')\n",
    "        out.write(result)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
